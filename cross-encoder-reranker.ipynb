{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0724986c975149866732d61b2ac90a7d138756189efb080eae0ea097c9e5b0cee",
   "display_name": "Python 3.7.6 64-bit ('nlp-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from utils import DocumentDB\n",
    "db = DocumentDB()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dhairya.dalal/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from src import SemanticIndex, BM25Index\n",
    "\n",
    "semantic_idx = SemanticIndex()\n",
    "bm25_idx = BM25Index()\n",
    "\n",
    "print(\"loaded\")"
   ]
  },
  {
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "ranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [i[0] for i in qrel[\"1\"].items()]\n",
    "texts = db.batch_docno_lookup(doc_ids)\n",
    "\n",
    "query = topics[0][\"title\"]\n",
    "pairs = list(zip([query] * len(texts), texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ranker.predict(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1101004_foreign_story_13015055.utf8 1 -5.9750423\n1101002_foreign_story_13006291.utf8 1 3.236979\n1100716_foreign_index.utf8 0 -1.0389109\n1100716_foreign_story_12689429.utf8 0 -0.79132426\n1100710_foreign_story_12667785.utf8 0 0.07188113\n1100616_foreign_story_12570790.utf8 1 3.850542\n1100510_opinion_story_12411827.utf8 1 1.4522578\n1100125_foreign_story_12025749.utf8 1 2.164837\n1091226_foreign_story_11910177.utf8 0 1.5910602\n1091130_foreign_index.utf8 0 -0.016197078\n1090915_foreign_index.utf8 0 0.08885631\n1090915_foreign_story_11495978.utf8 1 1.077204\n1090914_foreign_story_11490372.utf8 0 5.2121363\n1090724_foreign_story_11275868.utf8 0 4.704734\n1090604_foreign_index.utf8 1 -0.9948252\n1090604_foreign_story_11060253.utf8 1 0.7138958\n1090428_foreign_index.utf8 0 1.9561602\n1090428_foreign_story_10886383.utf8 0 5.082822\n1090118_foreign_story_10406051.utf8 0 1.2538654\n1081110_frontpage_story_10088589.utf8 1 0.7490781\n1080803_foreign_index.utf8 0 0.7565006\n1080710_foreign_story_9529901.utf8 0 2.0471585\n1080627_opinion_story_9470345.utf8 0 1.3212996\n1080625_foreign_story_9459686.utf8 0 -1.1259124\n1080402_foreign_story_9085363.utf8 1 1.025317\n1071231_foreign_story_8727957.utf8 1 4.5885587\n1071024_foreign_story_8465177.utf8 0 2.7067218\n1071024_foreign_story_8466133.utf8 1 1.8002167\n1070920_foreign_story_8339065.utf8 0 -0.702436\n1070917_foreign_story_8325687.utf8 0 1.3597789\n1070913_opinion_story_8308096.utf8 1 0.652485\n1070912_foreign_index.utf8 0 2.7413538\n1070912_foreign_story_8305358.utf8 1 4.589827\n1070909_foreign_story_8294775.utf8 1 0.9762206\n1070716_foreign_story_8063889.utf8 1 3.959119\n1070622_foreign_story_7957798.utf8 0 1.1349163\n1070618_nation_story_7937928.utf8 0 2.5709085\n1070204_foreign_story_7348048.utf8 0 -1.5657793\n1070201_foreign_story_7333687.utf8 0 5.363475\n1070112_foreign_story_7251782.utf8 1 1.894635\n1070105_foreign_story_7222808.utf8 0 1.7912743\n1061117_foreign_story_7013378.utf8 0 1.712842\n1061115_foreign_story_7003187.utf8 0 -1.0047319\n1060929_foreign_story_6808450.utf8 1 -0.35129166\n1060927_foreign_story_6798766.utf8 1 2.6097667\n1060925_foreign_story_6789860.utf8 1 2.4174976\n1060924_frontpage_story_6786713.utf8 0 2.4518013\n1060702_foreign_story_6427159.utf8 1 2.6929858\n1060701_foreign_story_6423722.utf8 1 4.4141693\n1060610_foreign_index.utf8 1 1.4687533\n1060609_foreign_story_6330449.utf8 0 3.0413792\n1060525_foreign_story_6267062.utf8 1 3.8749607\n1060507_foreign_story_6193172.utf8 0 -1.1687865\n1060428_opinion_story_6153506.utf8 0 -1.5611302\n1060424_foreign_story_6137737.utf8 1 1.0037699\n1060407_foreign_story_6068655.utf8 0 2.9541364\n1060302_foreign_story_5915016.utf8 1 -0.61408913\n1060226_foreign_story_5897727.utf8 1 2.1440125\n1060221_foreign_story_5875083.utf8 1 1.7478919\n1060122_foreign_story_5751917.utf8 1 -0.73886955\n1060120_foreign_story_5743717.utf8 1 1.0620209\n1060115_foreign_story_5723161.utf8 0 1.2184808\n1051208_foreign_story_5573753.utf8 1 3.238162\n1051204_foreign_story_5557293.utf8 0 4.3164783\n1051112_foreign_story_5468076.utf8 1 1.8813193\n1050930_foreign_index.utf8 0 -2.4781759\n1050930_foreign_story_5302687.utf8 1 2.4427133\n1050915_foreign_story_5240838.utf8 0 2.2610073\n1050809_foreign_story_5092336.utf8 0 0.36927035\n1050801_foreign_story_5059314.utf8 1 1.6041652\n1050715_calcutta_story_4989731.utf8 1 -3.739111\n1050706_foreign_story_4955544.utf8 0 2.5918531\n1050616_foreign_story_4874380.utf8 1 3.523448\n1050601_foreign_story_4811973.utf8 0 1.2441294\n1050506_foreign_story_4703322.utf8 0 3.192263\n1050505_foreign_story_4698765.utf8 0 1.4582856\n1050505_foreign_story_4698766.utf8 0 2.245482\n1050316_foreign_story_4499066.utf8 0 2.1160626\n1050315_foreign_story_4494493.utf8 0 -1.6649473\n1050225_foreign_story_4422498.utf8 1 0.65181816\n1041228_foreign_story_4183244.utf8 1 2.3083072\n1041226_foreign_story_4176042.utf8 0 1.2729099\n1041208_foreign_story_4100757.utf8 0 -0.9170494\n1041126_foreign_story_4051692.utf8 1 0.5606232\n1041031_foreign_story_3947441.utf8 1 1.545543\n1041031_frontpage_story_3947299.utf8 1 -0.5401078\n1041006_foreign_story_3847194.utf8 0 -7.383049\n1041002_foreign_story_3831817.utf8 0 2.4080925\n1040928_foreign_story_3813028.utf8 0 2.766561\n1040920_foreign_story_3779346.utf8 0 0.22669563\n1040813_foreign_story_3619808.utf8 0 2.6046686\n1040811_foreign_story_3610231.utf8 0 -3.9008703\n1040810_foreign_story_3606345.utf8 1 3.7597704\n1040809_foreign_story_3602118.utf8 1 1.1690524\n1040808_foreign_story_3598800.utf8 1 4.817566\n1040807_foreign_story_3595005.utf8 0 -0.24874017\n1040725_foreign_story_3539684.utf8 1 1.5951152\n1040719_foreign_story_3513628.utf8 1 1.8340378\n1040712_foreign_story_3484156.utf8 0 -0.45314842\n1040703_foreign_story_3448964.utf8 1 1.336979\n1040623_nation_story_3405084.utf8 0 0.73588204\n1040617_foreign_story_3381768.utf8 0 1.1363463\n1040614_foreign_story_3368458.utf8 0 2.9197931\n1040605_foreign_story_3334983.utf8 1 2.5555139\n1040526_foreign_story_3293645.utf8 1 2.0007296\n1040430_foreign_story_3191075.utf8 0 -1.4382324\n1040416_foreign_story_3133550.utf8 1 2.2029285\n1040412_foreign_story_3116420.utf8 1 -0.3999853\n1040409_foreign_story_3105514.utf8 0 -2.8951225\n1040406_foreign_story_3092091.utf8 0 1.3328815\n1040403_foreign_story_3082005.utf8 0 0.23190486\n1040402_foreign_story_3077319.utf8 1 2.1444728\n1040328_foreign_story_3056854.utf8 1 2.0991404\n1040327_foreign_story_3053307.utf8 0 1.9949243\n1040326_foreign_story_3049362.utf8 0 1.9923041\n1040325_foreign_story_3044510.utf8 1 3.7934947\n1040319_foreign_story_3022058.utf8 1 3.9300303\n1040319_frontpage_story_3022493.utf8 0 -0.00033937767\n1040317_foreign_story_3013650.utf8 0 0.23393585\n1040312_foreign_story_2994245.utf8 0 2.2613745\n1040305_foreign_story_2970006.utf8 0 3.236205\n1040302_foreign_story_2957662.utf8 0 2.0224175\n1040301_foreign_story_2953467.utf8 1 3.834125\n1040229_frontpage_index.utf8 0 2.5569854\n1040229_frontpage_story_2950585.utf8 1 2.8264759\n1040226_foreign_story_2939144.utf8 0 2.2052562\n1040223_foreign_story_2926876.utf8 0 2.8746\n1040213_foreign_story_2891333.utf8 1 -0.18715096\n1040124_foreign_story_2821302.utf8 0 1.6462462\n1040124_foreign_story_2821781.utf8 0 2.5908828\n1040107_foreign_story_2760257.utf8 1 -0.5670731\n1031222_foreign_story_2705676.utf8 0 0.73526514\n1031219_foreign_story_2695535.utf8 0 2.2577274\n1031215_foreign_story_2680371.utf8 0 -4.6515555\n1031118_foreign_story_2584216.utf8 1 -0.79145396\n1031110_foreign_story_2554965.utf8 0 0.59007424\n1031019_foreign_index.utf8 1 4.2943535\n1031019_foreign_story_2477149.utf8 0 2.3675432\n1031018_foreign_story_2474503.utf8 0 2.1892455\n1031015_foreign_story_2462680.utf8 0 2.3340168\n1031003_foreign_story_2425569.utf8 0 -3.2393472\n1030913_foreign_story_2361838.utf8 1 2.3495429\n1030912_foreign_story_2358489.utf8 0 0.07181722\n1030908_foreign_story_2344297.utf8 1 3.4018626\n1030901_foreign_story_2321166.utf8 1 1.9168109\n1030819_foreign_index.utf8 0 -2.3739297\n1030819_foreign_story_2277205.utf8 1 1.6255704\n1030815_foreign_story_2267415.utf8 0 0.9470803\n1030804_foreign_story_2230850.utf8 1 0.88235\n1030629_foreign_index.utf8 0 -0.132612\n1030629_foreign_story_2115081.utf8 0 5.0139494\n1030627_foreign_story_2108821.utf8 0 0.4345914\n1030530_foreign_story_2019661.utf8 1 -3.8879116\n1030523_foreign_story_1997125.utf8 1 -0.67660177\n1030522_foreign_story_1993630.utf8 1 3.483674\n1030521_foreign_story_1990209.utf8 1 1.1215435\n1030519_foreign_story_1982697.utf8 0 -0.48886213\n1030519_foreign_story_1982718.utf8 0 0.2305741\n1030518_foreign_story_1980348.utf8 1 1.3574641\n1030517_foreign_story_1977606.utf8 1 1.4328537\n1030503_foreign_story_1932772.utf8 1 3.203661\n1030325_foreign_story_1802569.utf8 1 2.6470323\n1030318_foreign_story_1779582.utf8 0 -0.5635173\n1030313_foreign_story_1762777.utf8 0 2.681783\n1030311_foreign_story_1755547.utf8 0 2.2855825\n1030310_foreign_story_1752424.utf8 0 2.96538\n1030310_foreign_story_1752425.utf8 0 -0.27960455\n1030308_foreign_story_1747032.utf8 0 3.8883696\n1030307_foreign_story_1743658.utf8 1 3.207333\n1030304_foreign_story_1732840.utf8 0 3.23388\n1030304_foreign_story_1732890.utf8 0 0.79303885\n1030303_foreign_story_1729261.utf8 0 1.0434567\n1030302_frontpage_story_1726852.utf8 0 2.000823\n1030227_foreign_story_1714857.utf8 0 1.3034637\n1030217_foreign_story_1679716.utf8 1 1.8537023\n1030217_frontpage_story_1679764.utf8 1 -0.75025237\n1030216_foreign_story_1676761.utf8 1 1.315807\n1030213_foreign_story_1666546.utf8 1 2.1490645\n1030212_foreign_story_1663404.utf8 1 0.2842832\n1030131_foreign_story_1625359.utf8 0 4.1391354\n1030130_foreign_story_1621318.utf8 0 0.73941433\n1030113_foreign_story_1567753.utf8 1 2.4502485\n1021209_foreign_story_1462594.utf8 1 -1.1924062\n1021205_foreign_story_1451138.utf8 0 0.85363024\n1021129_nation_story_1431942.utf8 0 0.99947983\n1021125_foreign_story_1417732.utf8 0 4.0117784\n1021119_foreign_story_1398992.utf8 1 2.0276785\n1021116_foreign_story_1390306.utf8 1 1.8652241\n1021115_foreign_story_1386926.utf8 1 2.9245074\n1021114_foreign_story_1383588.utf8 1 2.165893\n1021105_foreign_index.utf8 0 0.7885994\n1021104_foreign_index.utf8 0 -1.3332975\n1021104_foreign_story_1353117.utf8 0 -2.781138\n1021015_frontpage_story_1293377.utf8 0 0.33084518\n1021008_foreign_story_1273041.utf8 1 2.318184\n1021007_foreign_story_1269568.utf8 1 -7.284815\n1021004_foreign_story_1261140.utf8 0 5.2300835\n1020914_foreign_story_1198591.utf8 0 2.3811738\n1020912_foreign_story_1191613.utf8 1 -4.6940804\n1020911_foreign_story_1188707.utf8 1 -1.6936558\n1020908_foreign_story_1178943.utf8 1 2.8521214\n1020903_foreign_story_1162567.utf8 1 2.3893237\n1020830_nation_story_1151283.utf8 0 0.25636595\n"
     ]
    }
   ],
   "source": [
    "pos_ctr = 0\n",
    "neg_ctr = 0\n",
    "for i, v in enumerate(qrel[\"1\"].items()):\n",
    "    print(v[0], v[1], preds[i])\n",
    "    if v[1] == 0:\n",
    "        neg_ctr += 1\n",
    "    else:\n",
    "        pos_ctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "92 111\nAggregate results\nAverage MAP:  0.4934683496031204\nAverage P_5:  0.0\nAverage NDCG:  0.7914650633499363\n"
     ]
    }
   ],
   "source": [
    "print(pos_ctr, neg_ctr)\n",
    "run = {\"1\": dict(zip(doc_ids, preds.tolist()))}\n",
    "print_cum_stats(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aggregate results\nAverage MAP:  0.49188967040693543\nAverage P_5:  0.0\nAverage NDCG:  0.7910598955734557\n"
     ]
    }
   ],
   "source": [
    "p2 = [max(val, 0) for val in preds.tolist()]\n",
    "run = {\"1\": dict(zip(doc_ids, p2))}\n",
    "print_cum_stats(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "767267f8b5034a428749096000c72e5a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nAggregate results\nAverage MAP:  0.5326817223959618\nAverage P_5:  0.5799999999999998\nAverage NDCG:  0.8269792734077868\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "run = {}\n",
    "for topic in tqdm(topics):\n",
    "    number = topic[\"number\"]\n",
    "    query = topic[\"title\"]\n",
    "\n",
    "    extracted_ids = [k for k in qrel[number].keys()]\n",
    "\n",
    "    doc_ids = []\n",
    "    for id in extracted_ids:\n",
    "        try:\n",
    "            db.lookup_docno(id)\n",
    "            doc_ids.append(id)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    texts = db.batch_docno_lookup(doc_ids)\n",
    "    pairs = list(zip([query] * len(texts), texts))\n",
    "\n",
    "    scores = ranker.predict(pairs)\n",
    "    scores = scores.tolist()\n",
    "    \n",
    "    doc_scores = dict(zip(doc_ids, scores))\n",
    "    run[number] = doc_scores\n",
    "print_cum_stats(run)"
   ]
  },
  {
   "source": [
    "## Base CrossEncoder training w/ BCE Loss\n",
    "\n",
    "\n",
    "Average MAP:  0.6012651069594099\n",
    "\n",
    "Average P_5:  0.72\n",
    "\n",
    "Average NDCG:  0.8600546413378876\n",
    "\n",
    "\n",
    "## Constrastive Loss \n",
    "Aggregate results\n",
    "\n",
    "Average MAP:  0.5993319614460655\n",
    "\n",
    "Average P_5:  0.72\n",
    "\n",
    "Average NDCG:  0.8591562808100097\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Base loss\n",
    "from sentence_transformers import SentencesDataset, losses\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "examples = []\n",
    "\n",
    "for topic in topics:\n",
    "    gold = qrel[topic[\"number\"]].items()\n",
    "    query = topic[\"title\"].strip()\n",
    "\n",
    "    for item in gold:\n",
    "        try:\n",
    "            doc = db.lookup_docno(item[0])\n",
    "            examples.append(InputExample(texts=[query, doc], label=item[1]))\n",
    "        except:\n",
    "            continue\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cdf55626b024475aae139704f79f337"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=114.0, style=ProgressStyle(description_wi…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "822d888d2751462cbb6dff0228d7c1d7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 402653184 bytes. Error code 12 (Cannot allocate memory)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-cf5acc25607d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mranker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ranker/base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#train_loss =  losses.MultipleNegativesRankingLoss(model=ranker)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataloader, evaluator, epochs, loss_fct, activation_fct, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mskip_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mscale_before_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                     \u001b[0mmodel_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m         )\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    983\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m                 )\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         )\n\u001b[1;32m    463\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         )\n\u001b[1;32m    396\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 402653184 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = SentencesDataset(examples, ranker)\n",
    "train_dl = DataLoader(train_dataset, shuffle=True, batch_size = 24)\n",
    "ranker.fit(train_dataloader=train_dl, epochs=5,  output_path=\"ranker/base\")\n",
    "\n",
    "#train_loss =  losses.MultipleNegativesRankingLoss(model=ranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e35070d8ea494ef1b5c3fd0ef03f0026"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ddf99a3fd0994f2ba018b075651e8397"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9d964cdcc2645349407ab2af70116b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42e70442e53945a59cde573f8ec99318"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b387548756cd4b83951baacf6b71b001"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8395aeaa25f4ab198815a704c35c042"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df1fcbaa914543858267ac0cef664892"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0854655c7db543f3989ce7dc796f46ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e859aaff0b764841b3d9f6b70db672c3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "067693a6f2144550bc1f4a043ba84d60"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=5.0, style=ProgressStyle(description_widt…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a45c6aed6a543398ad6a2b01e0b8632"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n"
     ]
    }
   ],
   "source": [
    "ranker.fit(train_dataloader=train_dl, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [i[0] for i in qrel[\"20\"].items()]\n",
    "texts = db.batch_docno_lookup(doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = topics[4][\"title\"]\n",
    "\n",
    "pairs = list(zip([query] * len(texts), texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1.2180766 ,  2.380508  ,  3.7832723 , -0.6131114 ,  3.4686193 ,\n",
       "       -1.469696  ,  1.4555517 ,  1.4948552 , -1.248918  , -7.9772053 ,\n",
       "        4.381545  ,  0.86039066,  0.36753726,  4.004533  ,  1.6861217 ,\n",
       "        5.4812374 ,  4.6820188 ,  3.05331   ,  3.398644  , -4.625394  ,\n",
       "        3.3283222 ,  1.7538052 ,  1.1144562 ,  2.1483555 ,  3.0440204 ,\n",
       "        1.3770123 ,  3.139276  ,  5.4170923 ,  3.1966743 ,  1.1454346 ,\n",
       "        3.4415848 , -0.05710527,  0.9272184 ,  0.7655416 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "ranker.predict(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.47619975,  1.9613491 ,  3.4600346 ,  2.047238  ,  5.735491  ,\n",
       "        2.794967  ,  5.7426586 ,  6.668082  ,  5.684595  , -2.1745458 ,\n",
       "        4.680272  ,  6.2307715 ,  3.60517   ,  7.800994  ,  2.0835526 ,\n",
       "        1.2318438 ,  2.157736  , -0.14282535, -0.01890556], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "ranker.predict(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'number': '16',\n",
       "  'title': 'Telecom minister A. Raja resignation',\n",
       "  'narrative': 'Documents retrieved should contain information about the accusations against A. Raja in the 2G Spectrum scam. Documents that talk about his criticisms for allocating 4.4MHz of spectrum  a scarce resource  at the 2001 price of Rs 1,651 crore through a first come, first served policy rather than auctions; by compromising on the market value of spectrum, the government is estimated to have lost revenues worth around Rs 50,000 crore; new licensees are signing equity deals with foreign players in which the value of spectrum has risen sevenfold, would be considered as relevant.\\n'},\n",
       " {'number': '17',\n",
       "  'title': 'Brinda Karat accused Swami Ramdev',\n",
       "  'narrative': \"A relevant document should contain information about complaints made by Brinda Karat alleging that Ramdev has been selling medicines containing animal parts, thus violating licensing and labeling regulations laid down in India's Drug and Cosmetics Act.\"},\n",
       " {'number': '18',\n",
       "  'title': 'American government against abu ghraib prison military officers',\n",
       "  'narrative': \"Relevant documents should contain information about the american millitary officers' involvement in the inhuman torture meted out to the prisoners at the Abu Ghraib prison.\"},\n",
       " {'number': '19',\n",
       "  'title': 'Carphone Warehouse terminated deal with Channel 4',\n",
       "  'narrative': 'A relevant document should contain information about the racist attitude of Jade Goody towards Shilpa Shetty on the Celebrity Big Brother show, and as a consequence carphone warehouse refused to continue deal with channel 4'},\n",
       " {'number': '20',\n",
       "  'title': 'Shashi Tharoor resigned',\n",
       "  'narrative': 'Shashi Tharoor got involved in the IPL controversy, in a way that his friend Sunanda Pushkar was given free equity by IPL Kochi and he advised against prying into the consortiums ownership, Lalit Modi disclosed on Twitter. Arguments presented against him, and how these events lead up to his resignation as a Minister of Parliament would be considered as relevant documents. Any gossips related to his personal relationship with Sunanda Pushkar are not considerable.'}]"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}