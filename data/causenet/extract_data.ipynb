{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0724986c975149866732d61b2ac90a7d138756189efb080eae0ea097c9e5b0cee",
   "display_name": "Python 3.7.6 64-bit ('nlp-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "\n",
    "with open(\"causenet-precision.jsonl\", \"r\") as f:\n",
    "    json_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=197806.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7aa7124b167e4c3aab6709d32bdbd2cc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "extracted_examples = []\n",
    "\n",
    "for line in tqdm(json_lines, total = len(json_lines)):\n",
    "    line_json = json.loads(line)\n",
    "    cause = line_json[\"causal_relation\"][\"cause\"][\"concept\"].replace(\"_'\", \"'\").replace(\"_\",\" \")\n",
    "    effect = line_json[\"causal_relation\"][\"effect\"][\"concept\"].replace(\"_'\", \"'\").replace(\"_\",\" \")\n",
    "\n",
    "    example_sents = []\n",
    "    for ex in line_json[\"sources\"]:\n",
    "        if \"sentence\" in ex[\"payload\"]:\n",
    "            example_sents.append(ex[\"payload\"][\"sentence\"]) \n",
    "            break\n",
    "\n",
    "    for es in example_sents:\n",
    "        extracted_examples.append({\"cause\": cause, \"effect\": effect, \"text\": es.strip()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(extracted_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.15, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "\n",
    "def mark_sentence(s, match_list, word_dict):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \n",
    "              \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def tagged_text(text, cause, effect):\n",
    "    text = clean(r.text.lower())\n",
    "    word_dict = {}\n",
    "    tag_dict = {}\n",
    "\n",
    "    toks = word_tokenize(text)\n",
    "    pos = nltk.pos_tag(toks)\n",
    "\n",
    "    for i, tok in enumerate(toks):\n",
    "        word_dict[tok] = \"O\"\n",
    "        tag_dict[tok] = pos[i][1]\n",
    "  \n",
    "    match_list = []\n",
    "    annotations = [(r.cause, \"CAUSE\"), [r.effect, \"EFFECT\"]]\n",
    "    for k in annotations:\n",
    "        a, text_ = matcher(text, k[0])\n",
    "        match_list.append((a[0][0], a[0][1], k[1]))\n",
    "    tagged_seq = mark_sentence(text, match_list, word_dict)\n",
    "    return tagged_seq, tag_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "working on train.txt\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=138376.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c859f08529e489ba6734c7e98a98a9e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nignored:  0\nworking on val.txt\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=15376.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "734f93413e3b483ea023cd368703720a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nignored:  0\nworking on test.txt\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=27133.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e404e43c98684e2bb38483135f006ffc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nignored:  0\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "pairs = [(train, \"train.txt\"), (val, \"val.txt\"), (test, \"test.txt\")]\n",
    "\n",
    "bad_ctr = 0\n",
    "for pair in pairs:\n",
    "    print(f\"working on {pair[1]}\")\n",
    "    data = pair[0]\n",
    "    with open(pair[1], \"w\") as f:\n",
    "        for i,r in tqdm(data.iterrows(), total=len(data)):\n",
    "            try:\n",
    "                tagged_seq, pos = tagged_text(r.text, r.cause, r.effect)\n",
    "                for tag in tagged_seq.keys():\n",
    "                    f.writelines(tag + ' ' + pos[tag]  +' '+ tagged_seq[tag] + \"\\n\")\n",
    "                f.writelines(\"\\n\")\n",
    "            except:\n",
    "                bad_ctr+=1\n",
    "                continue\n",
    "    print(\"ignored: \", bad_ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}