{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0724986c975149866732d61b2ac90a7d138756189efb080eae0ea097c9e5b0cee",
   "display_name": "Python 3.7.6 64-bit ('nlp-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from utils import DocumentDB\n",
    "db = DocumentDB()\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dhairya.dalal/.conda/envs/nlp-env/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from src import SemanticIndex, BM25Index\n",
    "\n",
    "semantic_idx = SemanticIndex()\n",
    "bm25_idx = BM25Index()\n",
    "\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrec_eval\n",
    "from utils import read_qrel_from_file, evaluate_run, extract_topics_from_file\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "topics = extract_topics_from_file(\"qrels/2020/topics_test.txt\")\n",
    "qrel = read_qrel_from_file(\"qrels/2020/cair2020_qrel.txt\")\n",
    "\n",
    "metrics: set = {'map', 'ndcg', 'P_5'}\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrel, metrics)\n",
    "\n",
    "causitives = [\"because\", \"because of\", \"due to\", \"lead to\", \"causes\", \"result of\", \"leading cause\"]\n",
    "causitive_re = re.compile(r\"|\".join(causitives))\n",
    "\n",
    "ignore = [\"irrelevant\", \"not relevant\", \"does not satisfy relevance\"]\n",
    "ignore_re = re.compile(r\"|\".join(ignore))\n",
    "\n",
    "def print_cum_stats(run):\n",
    "    run_results = evaluator.evaluate(run)\n",
    "\n",
    "    map_scores = [v[\"map\"] for k,v in run_results.items()]\n",
    "    p_scores  = [v[\"P_5\"] for k,v in run_results.items()]\n",
    "    ndcg_scores = [v['ndcg'] for k,v in run_results.items()]\n",
    "\n",
    "    print(\"Aggregate results\")\n",
    "    print(\"Average MAP: \", np.mean(map_scores))\n",
    "    print(\"Average P_5: \", np.mean(p_scores))\n",
    "    print(\"Average NDCG: \", np.mean(ndcg_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def extract_key_words(text):\n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "    extractor.load_document(input=text, language='en')\n",
    "    extractor.candidate_selection()\n",
    "    extractor.candidate_weighting(threshold=.65)\n",
    "    kw = extractor.get_n_best(n=15)\n",
    "    return \" \".join([tup[0] for tup in kw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76e5e3485ef94fb6b834682bc2fa73a3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nAggregate results\nAverage MAP:  0.5774989891215204\nAverage P_5:  0.78\nAverage NDCG:  0.8580313074222803\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "title_run = {}\n",
    "narrative_run = {}\n",
    "combined_run = {}\n",
    "\n",
    "NUM_RESULTS = 1000\n",
    "for topic in tqdm(topics, total = len(topics)):\n",
    "    # Title results \n",
    "    title = topic[\"title\"].strip()\n",
    "    sti, sts = semantic_idx.query(title, num_results=NUM_RESULTS)\n",
    "    stid = db.batch_idx_lookup(sti)\n",
    "    stid_scores = dict(zip(stid, sts))\n",
    "\n",
    "    bti, bts = bm25_idx.query(title, num_results=NUM_RESULTS)\n",
    "    btid = db.batch_idx_lookup(bti)\n",
    "    btid_scores = dict(zip(btid, bts))\n",
    "\n",
    "    tctr = Counter(stid+btid)\n",
    "    t_scores = {}\n",
    "    for i in tctr:\n",
    "        s_score = stid_scores.get(i, 0)\n",
    "        b_score = btid_scores.get(i, 0)\n",
    "        t_scores[i] = s_score + b_score\n",
    " \n",
    "\n",
    "    # Narrative results \n",
    "    narrative = topic[\"narrative\"].strip()\n",
    "    rel_narrative = [sent for sent in sent_tokenize(narrative) if not ignore_re.search(sent) ]\n",
    "    rel_narrative = \" \".join(rel_narrative)\n",
    "    \n",
    "    narr_keywords = extract_key_words(rel_narrative)\n",
    "    \n",
    "    # sni, sns = semantic_idx.query(narr_keywords, num_results=30)\n",
    "    # snid = db.batch_idx_lookup(sni)\n",
    "    # snid_scores = dict(zip(snid, sts))\n",
    "\n",
    "    bni, bns = bm25_idx.query(narr_keywords, num_results=NUM_RESULTS)\n",
    "    bnid = db.batch_idx_lookup(bni)\n",
    "    bnid_scores = dict(zip(bnid, bts))\n",
    "\n",
    "    # nctr = Counter(snid+bnid)\n",
    "    # n_scores = {}\n",
    "    # for i in nctr:\n",
    "    #     s_score = snid_scores.get(i, 0)\n",
    "    #     b_score = bnid_scores.get(i, 0)\n",
    "    #     n_scores[i] = s_score + b_score\n",
    "\n",
    "    # combined_scores = {}\n",
    "    # all_keys = list(n_scores.keys()) + list(t_scores.keys())\n",
    "    # for key in all_keys:\n",
    "    #     t_s = t_scores.get(key, 0)\n",
    "    #     n_s = n_scores.get(key,0)\n",
    "    #     combined_scores[key] = t_s + n_s\n",
    "\n",
    "    combined_scores = {}\n",
    "    all_keys = list(t_scores.keys()) + list(bnid_scores.keys())\n",
    "    for key in all_keys:\n",
    "        t_s = t_scores.get(key, 0)\n",
    "        n_s = bnid_scores.get(key,0)\n",
    "        combined_scores[key] = np.mean([t_s,n_s])\n",
    "    combined_run[topic[\"number\"]] = combined_scores\n",
    "\n",
    "print_cum_stats(combined_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be236d429fc458eae549c4bc67821a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=8.0, style=ProgressStyle(description_width=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29ccaffbb5a8401493d5979f78ac84c3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=9.0, style=ProgressStyle(description_width=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b97904c124e48b9b98cd9e49b081487"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=6.0, style=ProgressStyle(description_width=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "783fbf156d4642a8994e471d22db9677"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=10.0, style=ProgressStyle(description_width…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15db1c3fa6f04cb9b7a63d7d46250678"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Batches', max=8.0, style=ProgressStyle(description_width=…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "241cbfe1433d4951a4927806227cc6a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nAggregate results\nAverage MAP:  0.48283078481786\nAverage P_5:  0.6\nAverage NDCG:  0.7545790605932867\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "title_run = {}\n",
    "narrative_run = {}\n",
    "combined_run = {}\n",
    "\n",
    "NUM_RESULTS = 100\n",
    "for topic in tqdm(topics, total = len(topics)):\n",
    "    # Title results \n",
    "    title = topic[\"title\"].strip()\n",
    "    sti, sts = semantic_idx.query(title, num_results=NUM_RESULTS)\n",
    "    stid = db.batch_idx_lookup(sti)\n",
    "    stid_scores = dict(zip(stid, sts))\n",
    "\n",
    "    bti, bts = bm25_idx.query(title, num_results=NUM_RESULTS)\n",
    "    btid = db.batch_idx_lookup(bti)\n",
    "    btid_scores = dict(zip(btid, bts))\n",
    "\n",
    "    tctr = Counter(stid+btid)\n",
    "    t_scores = {}\n",
    "    for i in tctr:\n",
    "        s_score = stid_scores.get(i, 0)\n",
    "        b_score = btid_scores.get(i, 0)\n",
    "        t_scores[i] = s_score + b_score\n",
    " \n",
    "\n",
    "    # Narrative results \n",
    "    narrative = topic[\"narrative\"].strip()\n",
    "    rel_narrative = [sent for sent in sent_tokenize(narrative) if not ignore_re.search(sent) ]\n",
    "    rel_narrative = \" \".join(rel_narrative)\n",
    "    \n",
    "    narr_keywords = extract_key_words(rel_narrative)\n",
    "    \n",
    "    # sni, sns = semantic_idx.query(narr_keywords, num_results=30)\n",
    "    # snid = db.batch_idx_lookup(sni)\n",
    "    # snid_scores = dict(zip(snid, sts))\n",
    "\n",
    "    bni, bns = bm25_idx.query(narr_keywords, num_results=NUM_RESULTS)\n",
    "    bnid = db.batch_idx_lookup(bni)\n",
    "    bnid_scores = dict(zip(bnid, bts))\n",
    "\n",
    "    # nctr = Counter(snid+bnid)\n",
    "    # n_scores = {}\n",
    "    # for i in nctr:\n",
    "    #     s_score = snid_scores.get(i, 0)\n",
    "    #     b_score = bnid_scores.get(i, 0)\n",
    "    #     n_scores[i] = s_score + b_score\n",
    "\n",
    "    # combined_scores = {}\n",
    "    # all_keys = list(n_scores.keys()) + list(t_scores.keys())\n",
    "    # for key in all_keys:\n",
    "    #     t_s = t_scores.get(key, 0)\n",
    "    #     n_s = n_scores.get(key,0)\n",
    "    #     combined_scores[key] = t_s + n_s\n",
    "\n",
    "    #combined_scores = {}\n",
    "    all_keys = list(t_scores.keys()) + list(bnid_scores.keys())\n",
    "    # for key in all_keys:\n",
    "    #     t_s = t_scores.get(key, 0)\n",
    "    #     n_s = bnid_scores.get(key,0)\n",
    "    #     combined_scores[key] = np.mean([t_s,n_s])\n",
    "    # combined_run[topic[\"number\"]] = combined_scores\n",
    "\n",
    "    all_keys = list(set(all_keys))\n",
    "    docs = db.batch_docno_lookup(all_keys)\n",
    "    inputs = [title] * len(docs)\n",
    "    pairs  = list(zip(inputs, docs))\n",
    "    preds = cross_encoder.predict(pairs,  show_progress_bar=True)\n",
    "    #preds = [max(v, 0) for v in preds.tolist()]\n",
    "    preds = preds.tolist()\n",
    "    \n",
    "    combined_scores = dict(zip(all_keys, preds))\n",
    "    combined_run[topic[\"number\"]] = combined_scores\n",
    "\n",
    "print_cum_stats(combined_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregate results\n",
    "Average MAP:  0.4106417897733161\n",
    "Average P_5:  0.52\n",
    "Average NDCG:  0.707158510865819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregate results\n",
    "Average MAP:  0.48283078481786\n",
    "Average P_5:  0.6\n",
    "Average NDCG:  0.7545790605932867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[101, 5604, 102]"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "tokenizer.encode(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] testing [SEP]'"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "tokenizer.decode([101, 5604, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}